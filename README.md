# 🦫 RAGondin — The Open RAG Experimentation Playground

RAGondin is a lightweight, modular and extensible Retrieval-Augmented Generation (RAG) framework designed to explore and test advanced RAG techniques — 100% open source and focused on experimentation, not lock-in.

> Built by the OpenLLM France community, RAGondin offers a sovereign-by-design alternative to mainstream RAG stacks like LangChain or Haystack.

## Goals
- Experiment with advanced RAG techniques
- Develop evaluation metrics for RAG applications
- Collaborate with the community to innovate and push the boundaries of RAG applications

## Current Features
This section provides a detailed explanation of the currently supported features.

The **`.hydra_config`** directory contains all the configuration files for the application. These configurations are structured using the [Hydra configuration framework](https://hydra.cc/docs/intro/). This directory will be referenced for setting up the RAG (Retrieval-Augmented Generation) pipeline.

### Supported File Formats
This branch currently supports the following file types:

* **TextFiles**: `txt`, `md`
* **Document Files**: `pdf`, `docx`, `doc`, `pptx`
* **Audio Files**: `wav`, `mp3`, `mp4`, `ogg`, `flv`, `wma`, `aac`

For all supported formats, content is converted to **Markdown**. Images within documents are replaced with captions generated using a **Vision Language Model (VLM)**. (See the **Configuration** section for more details.) The resulting Markdown is then chunked and indexed using the [Milvus vector database](https://milvus.io/).


> [!NOTE]
> **Upcoming support**: Future updates aim to include additional formats such as `csv`, `odt`, `html`, and image file types.

### Chunking
Several chunking strategies are supported, including **`semantic`** and **`recursive`**. By default, the **recursive chunker** is applied to all supported file types for its efficiency and low memory usage. This is the **recommended strategy** for most use cases. Future updates may introduce format-specific chunkers (e.g., for CSV, Markdown, etc.). The recursive chunker configuration is located at: *`.hydra_config/chunker/recursive_splitter.yaml`*.

```yml
# .hydra_config/chunker/recursive_splitter.yaml
defaults:
  - base
name: recursive_splitter
chunk_size: 1000
chunk_overlap: 100
```

The **`chunk_size`** and **`chunk_overlap`** values are expressed in **tokens**, not characters. For enhanced retrieval, enable the **contextual retrieval** feature—a technique introduced by Anthropic to improve retrieval performance ([Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval)). To activate it, set **`CONTEXTUAL_RETRIEVAL=true`** in your **`.env`** file. Refer to the **`Usage`** section for further instructions.


### **Indexing**

Once chunked, document fragments are ingested into the **Milvus** vector database using the `jinaai/jina-embeddings-v3` multilingual embedding model—top performer on the [MTEB benchmark](https://huggingface.co/spaces/mteb/leaderboard). To switch models, set the **`EMBEDDER_MODEL`** variable in your **`.env`** file to any Hugging Face–compatible alternative (e.g., `"sentence-transformers/all-MiniLM-L6-v2"` for faster throughput).

> \[!IMPORTANT]
> Choose an embedding model that aligns with your document languages and offers a suitable context window. The default model supports both English and French.

---

### **Retriever & Search**

#### Search Pipeline

After indexing, you can execute searches via our **hybrid search** pipeline, which blends **semantic search** with **BM25** keyword matching. This hybrid approach ensures you retrieve both topically relevant chunks and those containing exact keywords.

#### Retrieval Strategies

Three strategies feed into the hybrid search—available only in the **RAG pipeline** (see **OpenAI Compatible API**), not in the standalone **Semantic Search** endpoints:

* **multiQuery**: Uses an LLM to generate multiple query reformulations, merging their results for superior relevance (default and most effective based on benchmarks).
* **single**: Executes a single, straightforward retrieval query.
* **HyDE**: Generates a hypothetical answer via LLM, then retrieves chunks similar to that synthetic response.

---

#### **Reranker**

Finally, retrieved documents are re-ordered by relevance using a multilingual reranking model. By default, we employ **`jinaai/jina-colbert-v2`** from Hugging Face.

> [!IMPORTANT]
> The retriever fetches documents that are semantically similar to the query. However, semantic similarity doesn't always equate to relevance. Therefore, rerankers are crucial for reordering results and filtering out less pertinent documents, thereby reducing the likelihood of hallucinations.

### RAG Type
* **SimpleRAG**: Basic implementation without chat history taken into account.
* **ChatBotRAG**: Version that maintains conversation context. 

## 🚀 Getting Started

### 1. Clone the repository:
```bash
git clone https://github.com/OpenLLM-France/RAGondin.git
cd RAGondin
git checkout main # or a given release
```

### 2. Create uv environment and install dependencies:
>[!IMPORTANT] 
> Ensure you have Python 3.12 installed along with `uv`. For detailed installation instructions for uv, refer to the [uv official documentation](https://docs.astral.sh/uv/getting-started/installation/#pypi).

* To install `uv`, you can use either `pip` (if already available) or `curl`. Additional installation methods are outlined in the [documentation](https://docs.astral.sh/uv/getting-started/installation/#pypi).
```bash
# with pip
pip install uv

# with curl
curl -LsSf https://astral.sh/uv/install.sh | sh
```

```bash
# Create a new environment with all dependencies
cd RAGondin/
uv sync
```

### 3. Create a `.env` File

Create a `.env` file at the root of the project, mirroring the structure of `.env.example`, to configure your environment.

* Define your LLM-related variables (`API_KEY`, `BASE_URL`, `MODEL_NAME`) and VLM (Vision Language Model) settings (`VLM_API_KEY`, `VLM_BASE_URL`, `VLM_MODEL_NAME`).

> [!IMPORTANT]
> The **VLM** is used for generating captions from images extracted from files, and for tasks like summarizing chat history. The same endpoint can serve as both your LLM and VLM.

* For PDF indexing, multiple loader options are available. Set your choice using the **`PDFLoader`** variable:

  * **`MarkerLoader`** and **`DoclingLoader`** are recommended for optimal performance, especially on OCR-processed PDFs. They support both GPU and CPU execution, though CPU runs may be slower.
  * For lightweight testing on CPU, use **`PyMuPDF4LLMLoader`** or **`PyMuPDFLoader`**.

    > ⚠️ These do **not** support non-searchable PDFs or image-based content.


* Audio & Video Files
  - Audio and video content is transcribed using OpenAI’s **Whisper** model. Supported model sizes include: `tiny`, `base`, `small`, `medium`, `large`, and `turbo`. For details, refer to the [OpenAI Whisper repository](https://github.com/openai/whisper).The default transcription model is set via the **`WHISPER_MODEL`** variable, which defaults to `'base'`.

Other file formats (`txt`, `docx`, `doc`, `pptx`) are pre-configured.

* set `AUTH_TOKEN` to enable HTTP authentication via HTTPBearer. If not provided the endpoints will be accessible without any restrictions

```bash
# This is the minimal settings required.

# LLM settings
BASE_URL=
API_KEY=
MODEL=
LLM_SEMAPHORE=10

# VLM settings
VLM_BASE_URL=
VLM_API_KEY=
VLM_MODEL=
VLM_SEMAPHORE=10

# App
APP_PORT=8080
APP_HOST=0.0.0.0

# RAY
RAY_DEDUP_LOGS=0
RAY_DASHBOARD_PORT=8265

# To enable HTTP authentication via HTTPBearer for the api endpoints
AUTH_TOKEN=super-secret-token

# Loaders
PDFLoader=DoclingLoader

# Audio
WHISPER_MODEL=base

# Vector db VDB Milvus
VDB_HOST=milvus
VDB_CONNECTOR_NAME=milvus

# EMBEDDER
EMBEDDER_MODEL=jinaai/jina-embeddings-v3

# RETRIEVER
CONTEXTUAL_RETRIEVAL=true
RETRIEVER_TOP_K=12

# RERANKER
RERANKER_ENABLED=false
RERANKER_MODEL=jinaai/jina-reranker-v2-base-multilingual
RERANKER_MODEL_TYPE=crossencoder
RERANKER_TOP_K=5 # number of documents to return after reranking
```
* **Running on CPU**:
  For quick testing on CPU, you can reduce computational load by adjusting the following settings in the **`.env`** file:

- Set **`RERANKER_TOP_K=5`** or lower to limit the number of documents returned by the reranker. This defines how many documents are included in the LLM's context—reduce it if your LLM has a limited context window (4–5 is usually sufficient for an 8k context). You can also disable the reranker entirely with **`RERANKER_ENABLED=false`**, as it is a costly operation.

> [!WARNING]
> These adjustments may affect performance and result quality but are appropriate for lightweight testing.

* **Running on GPU**:
The default values are well-suited for GPU usage. However, you can adjust them as needed to experiment with different configurations based on your machine’s capabilities.

### 4.Deployment: Launch the app
The application can be launched in either in GPU or CPU environment, depending on your device's capabilities. Use the following commands:

```bash
# Launch with GPU support (recommended for faster processing)
docker compose up --build # or 'down' # to stop it

# Launch with CPU only
docker compose --profile cpu up --build # or '--profile cpu down' to stop it
```

Once it is running, you can check everything is fine by doing:
```bash
curl http://localhost:8080/health_check
```

> [!IMPORTANT]
> The initial launch is longer due to the installation of required dependencies. Once the application is up and running, you can access the fastapi documentation at `http://localhost:8080/docs` (8080 is the APP_PORT variable determined in your **`.env`**) to manage documents, execute searches, or interact with the RAG pipeline (see the **next section** about the api for more details). A default chat ui is also deployed using [chainlit](!https://docs.chainlit.io/get-started/overview). You can access to it at `http://localhost:8080/chainlit` chat with your documents with our RAG engine behind it.


### 5. Distributed deployment in a Ray cluster

To scale **RAGondin** in a distributed environment using **Ray**, follow the dedicated guide:

➡ [Deploy RAGondin in a Ray cluster](docs/deploy_ray_cluster.md)

### 6. 🧠 API Overview

This FastAPI-powered backend offers capabilities for document-based question answering (RAG), semantic search, and document indexing across multiple partitions. It exposes endpoints for interacting with a vector database and managing document ingestion, processing, and querying.

---
### 📍 Endpoints Summary

For all the following endpoints, make sure to include your authentication token **AUTH_TOKEN** in the HTTP request header if authentication is enabled.
---

#### 📦 Indexer

**`POST /indexer/partition/{partition}/file/{file_id}`**  
Uploads a file (with optional metadata) to a specific partition.

- **Inputs:**
  - `file` (form-data): binary – File to upload  
  - `metadata` (form-data): JSON string – Metadata for the file (e.g. `{"file_type": "pdf"}`)

- **Returns:**
  - `201 Created` with a JSON containing the task status URL
  - `409 Conflit` if a file with the same id in the same partition already exists

**`PUT /indexer/partition/{partition}/file/{file_id}`**  
Replaces an existing file in the partition. Deletes existing entry and creates a new indexation task.

- **Inputs:**
  - `file` (form-data): binary – File to upload  
  - `metadata` (form-data): JSON string – Metadata for the file (e.g. `{"file_type": "pdf"}`)

- **Returns:**
  - `202 Accepted` with a JSON containing the task status URL

**`PATCH /indexer/partition/{partition}/file/{file_id}`**  
Updates the metadata of an existing file without reindexing.

- **Inputs:**
  - `metadata` (form-data): JSON string – Metadata for the file (e.g. `{"file_type": "pdf"}`)

- **Returns:**
  - `200 Ok` if metadata for that file is successfully updated


---

**`DELETE /indexer/partition/{partition}/file/{file_id}`**  
Deletes a file from a specific partition.

- **Returns:**
  - `204 No content`
  - `404 Not found` if the file is not found in the partition


> [!NOTE]  
> Once the rag is running you can attack these endpoints in order to index multiple files. Check this [data_indexer.py](./utility/data_indexer.py) in the [ 📁utility](./utility/) folder.


---

**`GET /indexer/task/{task_id}`**  
Retrieves the status of an asynchronous indexing task (see the **`POST /indexer/partition/{partition}/file/{file_id}`** endpoint).


### 🔎 Semantic Search

**`GET /search/`**  
Searches across multiple partitions using a semantic query.

- **Inputs:**
  - `partitions` (query, optional): List[str] – Partitions to search (default: `["all"]`)  
  - `text` (query, required): string – Text to search semantically  
  - `top_k` (query, optional): int – Number of top results to return (default: `5`)

- **Returns:**
  - `200 OK` with a JSON list of document links (HATEOAS style)
  - `400 bad request` if the field `partitions` isn't correctly set

**`GET /search/partition/{partition}`**  
Searches within a specific partition.

- **Inputs:**
  - `text` (query, required): string – Text to search semantically  
  - `top_k` (query, optional): int – Number of top results to return (default: `5`)

- **Returns:**
  - `200 OK` with a JSON list of document links (HATEOAS style)
  - `400 bad request` if the field `partitions` isn't correctly set

**`GET /search/partition/{partition}/file/{file_id}`**  
Searches within a specific file in a partition.

- **Inputs:**
  - `text` (query, required): string – Text to search semantically  
  - `top_k` (query, optional): int – Number of top results to return (default: `5`)

- **Returns:**
  - `200 OK` with a JSON list of document links (HATEOAS style)
  - `400 bad request` if the field `partitions` isn't correctly set
---

#### 📄 Document Extract Details

**`GET /extract/{extract_id}`**  
Fetches a specific extract by its ID.

- **Returns:**
  - `content` and `metadata` of the extract (an extract is a chunk) inn JSON format

### 💬 OpenAI-Compatible Chat

OpenAI API compatibility enables seamless integration with existing tools and workflows that follow the OpenAI interface. It makes it easy to use popular UIs like **`OpenWebUI`** without the need for custom adapters.

For the following OpenAI-compatible endpoints, when using an OpenAI client, provide your `AUTH_TOKEN` as the `api_key` if authentication is enabled; otherwise, you can use any placeholder value such as `'sk-1234'`.

* **`GET /v1/models`**
This endpoint allows to list all existant **`models`**

> [!NOTE]  
> Model names follow the pattern **`ragondin-{partition_name}`**, where **`partition_name`** refers to a data partition containing specific files. These “models” aren’t standalone LLMs (like GPT-4 or Llama), but rather placeholders that tell your LLM endpoint to generate responses using only the data from the chosen partition. To query the entire vector database, use the special model name **`partition-all`**.


* **`POST /v1/chat/completions`**  
OpenAI-compatible chat completion endpoint using a Retrieval-Augmented Generation (RAG) pipeline. Accepts `model`, `messages`, `temperature`, `top_p`, etc.

* **`POST /v1/completions`**
Same for this endpoint


> [!TIP]
To test these endpoint with openai client, you can refer to the the [openai_compatibility_guide.ipynb](./utility/openai_compatibility_guide.ipynb) notebook from the [📁 utility](./utility/) folder
---

#### ℹ️ Utils

**`GET /health_check`**

Simple endpoint to ensure the server is running.

---

## 🏗️ Architecture

```mermaid
graph TD
    A[User Query] --> B[RAGondin Backend]
    B --> C[Retriever (Milvus)]
    B --> D[Chunker & Preprocessing]
    C --> E[Relevant Chunks]
    E --> F[LLM Completion]
    F --> G[Final Answer]
```

🧩 Designed for plug & play: each component can be swapped independently.

---

## 🔧 Contributing

We ❤️ contributions!

```bash
error: Distribution `ray==2.43.0 @ registry+https://pypi.org/simple` can't be installed because it doesn't have a source distribution or wheel for the current platform

Read our [CONTRIBUTING.md](CONTRIBUTING.md) for coding standards, test setup, and more.

---

## 📄 License

## TODO
[] Better manage logs
